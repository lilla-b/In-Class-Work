{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9edcbb51",
   "metadata": {},
   "source": [
    "# Week 14 Group Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f800b1c",
   "metadata": {},
   "source": [
    "1. Using the documentation for Recursive Feature Selection, apply this process to the crime dataset to create the best multivariate linear regression model https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html.\n",
    "You can select what you're trying to predict, but be sure to indicate what that is. Be sure to explain what RFE is in the markdown. You shoud be able to answer this using what's on the documentation page and what you already know. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910d9e16",
   "metadata": {},
   "source": [
    "The goal of Recursive Feature Selection (RFE) to select a minimum set of informative features for use in a ML model. With RFE, this is accomplished by instantiating a supervised learning estimator and a number of desired features. The estimator \"weights\" features on a training data set. Features are then ranked, and the 'least important' (based on some criterion) are dropped from the set. This is performed iteratively until the minimum set (number of desired features) is achieved. \n",
    "\n",
    "Why do we care? One way to strengthen a signal in our data set (while not creating the artifical signals that arise from overfitting) is to reduce our data set to the smallest set of highly informative features.  Recursive Features Selection is one tool that allows us to approach that goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af434725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import AdaBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7977dee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>478</td>\n",
       "      <td>184</td>\n",
       "      <td>40</td>\n",
       "      <td>74</td>\n",
       "      <td>11</td>\n",
       "      <td>31</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>494</td>\n",
       "      <td>213</td>\n",
       "      <td>32</td>\n",
       "      <td>72</td>\n",
       "      <td>11</td>\n",
       "      <td>43</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>643</td>\n",
       "      <td>347</td>\n",
       "      <td>57</td>\n",
       "      <td>70</td>\n",
       "      <td>18</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>341</td>\n",
       "      <td>565</td>\n",
       "      <td>31</td>\n",
       "      <td>71</td>\n",
       "      <td>11</td>\n",
       "      <td>25</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>773</td>\n",
       "      <td>327</td>\n",
       "      <td>67</td>\n",
       "      <td>72</td>\n",
       "      <td>9</td>\n",
       "      <td>29</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    X1   X2  X3  X4  X5  X6  X7\n",
       "0  478  184  40  74  11  31  20\n",
       "1  494  213  32  72  11  43  18\n",
       "2  643  347  57  70  18  16  16\n",
       "3  341  565  31  71  11  25  19\n",
       "4  773  327  67  72   9  29  24"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crime_df = pd.read_csv(\"../week_13/crime_data.csv\")\n",
    "crime_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82a69fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask data:  [ True False False False False False]\n",
      "Ranking:  [1 2 3 5 6 4]\n"
     ]
    }
   ],
   "source": [
    "# Here we used AdaBoostRegressor meta-estimator model. We want the RFE to select only one feature.\n",
    "\n",
    "X = crime_df.drop('X1', axis=1)\n",
    "y = crime_df['X1']\n",
    "\n",
    "estimator = AdaBoostRegressor(random_state=0, n_estimators = 100)\n",
    "selector = RFE(estimator, n_features_to_select = 1, step = 1)\n",
    "selector = selector.fit(X, y)\n",
    "\n",
    "filter = selector.support_\n",
    "ranking = selector.ranking_\n",
    "\n",
    "print(\"Mask data: \", filter)\n",
    "print(\"Ranking: \", ranking)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0489ec1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All features: \n",
      "['X2' 'X3' 'X4' 'X5' 'X6' 'X7']\n",
      "Selected features: \n",
      "['X2']\n"
     ]
    }
   ],
   "source": [
    "# Display selected features:\n",
    "\n",
    "features = np.array(X.columns)\n",
    "print('All features: ')\n",
    "print(features)\n",
    "\n",
    "print('Selected features: ')\n",
    "print(features[filter])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9931294",
   "metadata": {},
   "source": [
    "2. Create a list of preprocessing steps you should try when working to build a model. Briefly describe what each step is. Work with your group to come up with the most comprehensive list you can."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21af0b5",
   "metadata": {},
   "source": [
    "We came up with a nice long set of steps as a group in class but my notebook did not save properly, my answer will not match my those of other group members.\n",
    "\n",
    "Preprocessing tasks fall into these categories: Preparation/cleaning, Feature Selection, Transformation, Feature Engineering, and Dimension Reduction\n",
    "\n",
    "_Preparation/cleaning:_ In this stage, we deal with incomplete, noisy, and/or inconsistent records. Tasks include:\n",
    "    - Validate data types \n",
    "    - Correct inconsistent data\n",
    "    - Remove or replace uninformative characters \n",
    "    - Remove outliers\n",
    "    - Handle null values (can be tricky to spot, e.g. '?'). Remove or replace with imputation technique (e.g. replace NaN with mean). sklearn has a class, SimpleImputer, that performs this substitution.\n",
    "    - Smooth noisy data. Can use plots (box, scatter) to identify outliers for numeric variables. Can also use techniques such as binning, regression, or clustering (also for outlier detection).\n",
    "\n",
    "_Feature Selection:_ Here we identify and select the most informative features. We do this to improve the SNR, minimize overfitting, and reduce computation time. The Recursive Feature Elimination (RFE) algorithm, which removes redundant inpute variables, is widely used for this purpose. \n",
    "\n",
    "_Data Transformation:_  This set of tasks \"massages\" the data into forms that have properties are amenable to modeling. Tasks include:\n",
    "    - Normalization - a type of re-scaling where all values are between 0-1 (inclusive). Removes \"weighting\" effect that can result with features that are measured on differet scales (e.g. mass and age). Can use MinMaxScaler. \n",
    "    - Standardization. Linear models operate on the assumption that random variables are normally distributed with equal variances. We rescale all of our variables so that the distribution is centered on a mean of zero and a variance of one.We accomplish standardizing using sklearn's StandardScaler. Both training and testing data sets should be standardized. \n",
    "    Discretization, the transformation of numerical variables into discrete categories (if we were using a model that wanted categorical features). \n",
    "    \n",
    "_Feature Engineering:_  creation of new, higher-level or numerical features from raw input features. Examples include: \n",
    "    - Handling categorical values. (Ordinal and nominal values require different approaches.)\n",
    "        - Ordinal: create dataframe of ordinals. Then, either create a dictionary and use a .map() function, or use the LabelEncoder class.\n",
    "            - Nominal: Use One-Hot Encoding, either by hand or by using .get_dummies().\n",
    "        Prior to applying a One-Hot Encoding technique we need to examine for multicollinearity, a property of a feature set that can influece model behavior and distort interpretation of results. The simplest way is to generate a correlation matrix and drop one of a pair of strongly correlated features. \n",
    "\n",
    "_Dimension reduction:_  removal of features that are redundant or of low relevance. We can achieve using Principal Component Analysis (PCA).\n",
    "    \n",
    "We can also perform other tasks to reduce the data set, such as filtering based on certain criteria (usually a threshold). For example, _Missing values ratio_ - removal of features with missing values exceeding a specified threshold\n",
    "    _Low variance filter_ - removal of normalized features that have variance below a certain threshold (low variation - low information)\n",
    "    _High Correlation filter_ - removal of normalized features that have correlation coefficients above a certain threshold. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
